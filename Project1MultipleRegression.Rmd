---
title: "Project 1 Multiple Regression"
author: "Jefferson Ong, Jayne Hollar, Céline Prunet"
date: "3/15/2020"
output: html_document
---


```{r , comment= NA, warning= F, message = FALSE}
library(readxl)
library(tidyverse)
house <- read_excel("Housing.xlsx")
```

# Data Summary: 

Examine the statistics and values for each variable.  Are any missing?  Do any values need clarification or modification?  If so, why and what did you do?

```{r, comment = NA}
str(house)
summary(house)
```

* Here is a quick look at the dataset


```{r, comment= NA}
which(is.na(house))
```



* None are missing in the data frame

## Do any values need clarification or modification?

* Yes status should be a factor rather than just characters. 

```{r, comment= NA}
unique(house$status)
unique(house$elem)
```

* We needed to see all the different elements in these variables. 

```{r, comment= NA}

house$status <- as.factor(c("sld" = "1", "pen" = "2", "act" = "3")[house$status])
house$status
house$elem <- as.factor(c("edison" = "1", "adams" = "2", "parker" = "3", "edge" = "4", "harris" = 5, "crest" = 6)[house$elem])
house$elem
```

* Here we changed status of the house into a factor of three levels where sld = 1, meaning 1 when the house is sold, pen = 2 where the house is pending sale, and act = 3 where house is in the active listing. We did the same for the elementary school distrinct

_____


# Exploratory Data Analysis:  

## Scatterplot

```{r, comment= NA}
plot(house[1:10])
```

* If we look at the second row of this multi-scatterplot, we can see the relationship of price as y against all the other variables individually. We can see that bath, bedroom, garagesize, and status are discrete variables. We can consider them as factors where a house's bathroom will either have 1, 2, or 3 bathrooms but for now we won't, in an extreme case of a house having more than 3 bathroom. Where as a house can only have three states. 

## Correlation plot

The variables: size, lot, bedrooms, yearbuilt/agestandardized has outliers

```{r, comment= NA, warning= F, message = FALSE}
cor(house[, c(1:9)])

library("PerformanceAnalytics")
chart.Correlation(house[, c(1:9)], histogram=TRUE, pch=19)
```
                         id       price       size         lot       bath    bedrooms   yearbuilt agestandardized garagesize
                         
price            0.04662108  1.00000000 0.20143783  0.24423228  0.1746578 -0.28619746  0.15412476      0.15412476  0.3583861

* Here we can see that price as a single predictor against the other variables. We can see that individually each predictor doesn't correlate strongly to price. 


Examine some of the variables relationships with price to help you determine which variables might be useful in an initial model.  Explain your conclusions from this initial screening.

* We can see that the promising predictor variables are garagesize, lot, bedrooms(negatively). We can look further in size, bath, yearbuilt/agestandardized if we need more predictors. It makes sense that id doesn't predict anythign since its just an index and that yearbuilt and agestandardized are the same since agestandardized is just a transformation of yearbuilt. 

* We wanted to see why bedroom is negatively correlated since it's not intuitive that having more bedrooms mean that the house is a lower price. We can see from the scatterplot that houses that have 4 or more bedrooms typically cost less than the 2 or 3 bedroom counterparts. 

```{r}
sample <- house %>%
  select(bedrooms, price) %>%
  arrange(bedrooms)
plot(sample)
```

## Outliers


```{r, comment= NA, warning= F, message = FALSE}

OutVals = boxplot(house$size, plot = T)$out
which(house$size %in% OutVals)
OutVals = boxplot(house$lot, plot = T)$out
which(house$lot %in% OutVals)
OutVals = boxplot(house$bath, plot = T)$out
which(house$bath %in% OutVals)
OutVals = boxplot(house$bedrooms, plot = T)$out
which(house$bedrooms %in% OutVals)
OutVals = boxplot(house$yearbuilt, plot = T)$out
which(house$yearbuilt %in% OutVals)
OutVals = boxplot(house$garagesize, plot = T)$out
which(house$garagesize %in% OutVals)
```

* We went ahead and look at size, lot, bath, bedroom, yearbuilt, and garagesize for outliers since id is just an index, price because it's what we're tryin to predict, and agestandardized as it will give the same as yearbuilt. Boxplot would not be appropriate in for status and elem since it wouldn't make sense. 


```{r, comment= NA, warning= F, message = FALSE}
house$size[c(4, 20, 76)]
house$lot[c(74)]
#house$bath 
house$bedrooms[c(35)]
house$yearbuilt[c(35, 52, 54, 64)]
#house$garagesize
```



* We then looked at the individual points to see if the data can explain why these particular points are outliers. We found that houses built before the 1920 has the 6 bedroom home which explains some of the negative correlation between price and bedroom numbers. An intuition would be that older homes, especially those before the 1920s will cost more and have more bedrooms. 

```{r, comment= NA, warning= F, message = FALSE}
house[c(74, 76),]
```

* We then looked at the row 74 and 76 since 74 had a particuarly large lot and 76 had a huge size. We found that theres nothing apparent that would explain this deviation and decided to remove them because of this. They aren't representative of the data at hand. We will also remove id as it isn't needed for our analysis

```{r, comment= NA, warning= F, message = FALSE}
house <- house[-c(74,76), ]
house <- subset(house, select = -id )
str(house)
```

_________

# Initial Modeling:  



```{r, comment= NA, warning= F, message = FALSE}
model1 <- lm(price ~. -agestandardized , house)
summary(model1)
```
Using your conclusions from the exploratory data analysis, build a regression model and report your findings.

* We can see that the model isn't just random and does explain the price in someway, by the p-value on the F-statistic. 

          F-statistic: 5.118 on 13 and 59 DF,  p-value: 5.685e-06.
          
* The model also explains about 53% of the variability in price, using the Multiple R-squared: 0.53. however this is inflated due to the sheer amount of predictors that we are using, looking at the Adjusted R-squared:  0.4264 means that it should only be about 42%.

* We can see that most of the significant predictors depend on the school district and status 3(whether the house is listed). 

* There is also NA values for the agestandardized which makes sense since it's the same as yearbuilt and adds nothing to the model.

______


# Model Modification:  

Consider modifying your model based upon your initial results and/or diagnostic plots from the model.  Explain any modifications that you are making.  Consider variance inflation factors for each predictor in your model and comment on that in your model selection process.



```{r, comment= NA, warning= F, message = FALSE}
library(car)
vif(model1)
```

* Using the GVIF^(1/(2*Df)) since elem has 5 coefficients and status has df 2, for all the different factors. There isn't much multicollinearity between the predictor variables.


```{r, comment= NA, warning= F, message = FALSE}
model2 <- lm(price ~. -agestandardized -yearbuilt -bath -lot -garagesize, house)
summary(model2)
vif(model2)
```

* We then started to remove predictor variables base on the highest p value on their individual t statistic. While also keeping in mind the vif at each iteration on whether one would show multicollinearity, this is not occur. After removing yearbuilt and bath we saw size became significant. When we removed lot bedroom started to show that it might be significant and finally removing garagesize, we end up with a model where all the predictors are significant. 

 
* The p value on the f-statistic shows us this is a significant model overall with a R^2 of 49%, adjusted of 41%. 

```{r, comment= NA, warning= F, message = FALSE}
par(mfrow = c(2,2))
plot(model2)
```

```{r, comment= NA, warning= F, message = FALSE}
house[4,]
```

* Again we see that this is the house with 6 bedrooms that we decided to keep earlier on.

* We can see from the plot diagnostics that we aren't violating any underlying assumptions for linear regression modelling. 


```{r, comment= NA, warning= F, message = FALSE}
model3 <- lm(price ~. -agestandardized -yearbuilt -bath -lot -garagesize + agestandardized:size, house)
summary(model3)
par(mfrow = c(2,2))
plot(model3)
```

* Here we've looked at the interaction term that we thought we be significant earlier from looking at the data. We find out that is it not. 

```{r, comment= NA, warning= F, message = FALSE}
model4 <- lm(log(price) ~. -agestandardized -yearbuilt -bath -lot -garagesize, house)
summary(model4)
par(mfrow = c(2,2))
plot(model4)
```

* We applied a transformation on the Y response variable, seeing if it will yield to better plot diagnostics. It doesn't change much for the most part aside from the residual vs fitted plot being more spread out but curving more drastically. 

```{r, comment= NA, warning= F, message = FALSE}
model4 <- lm(price ~ elem + status + I(size /100) + bedrooms, house)
summary(model4)
par(mfrow = c(2,2))
plot(model4)
```

* We applied a transformation on size to see changes in the plot diagnostics. We can see that changes in the predictor variables does not have any affects on it's significances. 

_____


# Conclusions: 

Present your final model and diagnostic plots in support of that final model.  In that presentation of the final model, comment on the R-squared value and its interpretation, give 95% confidence intervals for each of the β coefficients in your model, and illustrate your model’s use with a 95% confidence interval for the mean response and a 95% prediction interval for individual response for a hypothetical house of your choosing.


```{r, comment= NA, warning= F, message = FALSE}
modelf <- lm(price ~ elem + status + size + bedrooms, house)
summary(modelf)
par(mfrow = c(2,2))
plot(modelf)
```


* After looking at the transformation on the on the response, interaction terms, and some of the predictor variables. We did not find anything compelling that would improve the model from what we already had. 

* The R^2 value is 0.4909 which mean that the model explains about 49% of the variation. Looking at the adjusted R^2 of 0.4193 we're penalized by the amount of variables in the model. 

```{r, comment= NA, warning= F, message = FALSE}
confint(modelf)
```

* This is the β coefficients in your model, we are 95% confidence that the actual point is between these numbers. 

* We can be 95% confident that the actual value for elem2 is between -147.083452 to -29.326497.

* We can be 95% confident that the actual value for elem3 is between -125.974296 to -54.054080.

* We can be 95% confident that the actual value for elem4 is between -110.188716 to -46.906813.

* We can be 95% confident that the actual value for elem5 is between -76.117722  to -2.368995.

* We can be 95% confident that the actual value for elem6 is between -127.375332 to -27.352433.

* We can be 95% confident that the actual value for status2 is between  2.627647 to 62.494973.

* We can be 95% confident that the actual value for status3 is between 19.442006 to 71.999903.

* We can be 95% confident that the actual value for size is between 15.759564 to 141.087464.

* We can be 95% confident that the actual value for bedrooms is between -35.326602 to -4.198871.




```{r, comment= NA, warning= F, message = FALSE}
newhouse <- data.frame(size = 2, bedrooms  = 4, status = "1", elem = "3")
predict(modelf, newdata = newhouse, interval = 'confidence')
```

* 225.751 is the point estimate for this particular house
* Base on the model, a house with these predictors will have a price of 199.6205 to 251.8815


```{r, comment= NA, warning= F, message = FALSE}
newhouse <- data.frame(size = 2, bedrooms  = 4, status = "1", elem = "3")
predict(modelf, newdata = newhouse, interval = 'predict')
```

* 230.8879 is the point estimate for this particular house
* Base on the model, 95% of prices with these predictors will have a price of 135.1932 to 326.5827




















